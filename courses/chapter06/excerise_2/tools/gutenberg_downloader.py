#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Project Gutenberg ç»Ÿä¸€ä¹¦ç±ä¸‹è½½å™¨
æ”¯æŒä»é…ç½®æ–‡ä»¶å’Œåœ¨çº¿æºè·å–ä¹¦ç±åˆ—è¡¨ï¼Œç”¨äºMapReduce Word Countæ€§èƒ½æµ‹è¯•

ç‰¹æ€§:
- æ”¯æŒJSONé…ç½®æ–‡ä»¶ç®¡ç†ä¹¦ç±åˆ—è¡¨
- æ”¯æŒå¤šç§ä¹¦ç±é›†åˆï¼ˆessential, extended, mega, large_worksï¼‰
- æ”¯æŒä»ç½‘ç«™åŠ¨æ€è·å–ä¹¦ç±åˆ—è¡¨
- æ™ºèƒ½é‡è¯•å’Œé”™è¯¯å¤„ç†
- è¿›åº¦æ˜¾ç¤ºå’Œç»Ÿè®¡ä¿¡æ¯
- è‡ªåŠ¨åˆå¹¶åŠŸèƒ½
"""

import os
import json
import urllib.request
import urllib.error
import time
import sys
import argparse
import re
from pathlib import Path
from urllib.parse import urljoin
from typing import Dict, List, Optional, Tuple

class GutenbergDownloader:
    """Project Gutenberg ä¹¦ç±ä¸‹è½½å™¨"""
    
    def __init__(self, config_file: str = "book_catalog.json"):
        """åˆå§‹åŒ–ä¸‹è½½å™¨
        
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_file = config_file
        self.config = self._load_config()
        self.download_settings = self.config.get("download_settings", {})
        self.stats = {
            "total": 0,
            "downloaded": 0,
            "failed": 0,
            "skipped": 0,
            "total_size": 0
        }
    
    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"âŒ é…ç½®æ–‡ä»¶ {self.config_file} ä¸å­˜åœ¨!")
            sys.exit(1)
        except json.JSONDecodeError as e:
            print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            sys.exit(1)
    
    def _create_output_directory(self) -> Path:
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        output_dir = Path(self.download_settings.get("output_directory", "data/books"))
        output_dir.mkdir(parents=True, exist_ok=True)
        return output_dir
    
    def _clean_filename(self, text: str) -> str:
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦"""
        # ç§»é™¤æˆ–æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[<>:"/\\|?*]', '', text)
        text = re.sub(r'[^\w\s-]', '', text)
        text = re.sub(r'[-\s]+', '_', text)
        return text.strip('_')
    
    def _download_book(self, book_info: Dict, output_dir: Path) -> Tuple[bool, int]:
        """ä¸‹è½½å•æœ¬ä¹¦ç±
        
        Args:
            book_info: ä¹¦ç±ä¿¡æ¯å­—å…¸
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            (æ˜¯å¦æˆåŠŸ, æ–‡ä»¶å¤§å°)
        """
        book_id = book_info["id"]
        title = book_info["title"]
        author = book_info["author"]
        
        # ç”Ÿæˆæ–‡ä»¶å
        clean_title = self._clean_filename(title)
        clean_author = self._clean_filename(author)
        filename = f"{book_id}_{clean_title}_{clean_author}.txt"
        filepath = output_dir / filename
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if filepath.exists():
            file_size = filepath.stat().st_size
            print(f"  â­ï¸  è·³è¿‡å·²å­˜åœ¨: {filename} ({file_size/1024:.1f} KB)")
            self.stats["skipped"] += 1
            return True, file_size
        
        # å°è¯•ä¸‹è½½
        urls = [
            self.download_settings["base_url"].format(id=book_id),
            self.download_settings["fallback_url"].format(id=book_id)
        ]
        
        retry_attempts = self.download_settings.get("retry_attempts", 3)
        
        for attempt in range(retry_attempts):
            for url in urls:
                try:
                    print(f"  ğŸ“¥ ä¸‹è½½ä¸­: {title} (å°è¯• {attempt + 1}/{retry_attempts})")
                    
                    with urllib.request.urlopen(url, timeout=30) as response:
                        content = response.read().decode('utf-8', errors='ignore')
                        
                        # å†™å…¥æ–‡ä»¶
                        with open(filepath, 'w', encoding='utf-8') as f:
                            f.write(content)
                        
                        file_size = len(content.encode('utf-8'))
                        print(f"  âœ… å®Œæˆ: {filename} ({file_size/1024:.1f} KB)")
                        self.stats["downloaded"] += 1
                        self.stats["total_size"] += file_size
                        
                        # ä¸‹è½½é—´éš”
                        delay = self.download_settings.get("delay_between_downloads", 1)
                        if delay > 0:
                            time.sleep(delay)
                        
                        return True, file_size
                        
                except urllib.error.HTTPError as e:
                    if e.code == 404:
                        print(f"  âš ï¸  URLä¸å­˜åœ¨ (404): {url}")
                        continue
                    else:
                        print(f"  âŒ HTTPé”™è¯¯ {e.code}: {url}")
                        
                except Exception as e:
                    print(f"  âŒ ä¸‹è½½å¤±è´¥: {e}")
                    
                # é‡è¯•å‰ç­‰å¾…
                if attempt < retry_attempts - 1:
                    time.sleep(2)
        
        print(f"  âŒ ä¸‹è½½å¤±è´¥: {title}")
        self.stats["failed"] += 1
        return False, 0
    
    def download_collection(self, collection_name: str) -> bool:
        """ä¸‹è½½æŒ‡å®šçš„ä¹¦ç±é›†åˆ
        
        Args:
            collection_name: é›†åˆåç§° (essential, extended, mega, large_works)
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        collections = self.config.get("book_collections", {})
        
        if collection_name not in collections:
            print(f"âŒ æœªæ‰¾åˆ°é›†åˆ: {collection_name}")
            print(f"å¯ç”¨é›†åˆ: {list(collections.keys())}")
            return False
        
        collection = collections[collection_name]
        books = collection.get("books", [])
        
        if not books:
            print(f"âŒ é›†åˆ {collection_name} ä¸­æ²¡æœ‰ä¹¦ç±")
            return False
        
        print(f"ğŸ“š å¼€å§‹ä¸‹è½½é›†åˆ: {collection_name}")
        print(f"ğŸ“– æè¿°: {collection.get('description', '')}")
        print(f"ğŸ¯ ç›®æ ‡å¤§å°: {collection.get('target_size_mb', 0)} MB")
        print(f"ğŸ“Š ä¹¦ç±æ•°é‡: {len(books)} æœ¬")
        print("=" * 60)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = self._create_output_directory()
        
        # é‡ç½®ç»Ÿè®¡
        self.stats = {
            "total": len(books),
            "downloaded": 0,
            "failed": 0,
            "skipped": 0,
            "total_size": 0
        }
        
        start_time = time.time()
        
        # ä¸‹è½½ä¹¦ç±
        for i, book in enumerate(books, 1):
            progress = (i / len(books)) * 100
            print(f"[{progress:5.1f}%] ({i}/{len(books)})")
            
            success, file_size = self._download_book(book, output_dir)
            
            if not success:
                continue
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        end_time = time.time()
        processing_time = end_time - start_time
        
        print("\n" + "=" * 60)
        print("âœ… ä¸‹è½½å®Œæˆ!")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - æ€»ä¹¦ç±æ•°: {self.stats['total']} æœ¬")
        print(f"   - æˆåŠŸä¸‹è½½: {self.stats['downloaded']} æœ¬")
        print(f"   - è·³è¿‡å·²å­˜åœ¨: {self.stats['skipped']} æœ¬")
        print(f"   - ä¸‹è½½å¤±è´¥: {self.stats['failed']} æœ¬")
        print(f"   - æ€»å¤§å°: {self.stats['total_size']/1024/1024:.1f} MB")
        print(f"   - å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
        print(f"   - è¾“å‡ºç›®å½•: {output_dir}")
        
        return True
    
    def merge_books(self, output_file: Optional[str] = None) -> bool:
        """åˆå¹¶æ‰€æœ‰ä¸‹è½½çš„ä¹¦ç±ä¸ºä¸€ä¸ªæ–‡ä»¶
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if output_file is None:
            output_file = self.download_settings.get("merge_output", "data/all_books_merged.txt")
        
        output_path = Path(output_file)
        books_dir = Path(self.download_settings.get("output_directory", "data/books"))
        
        if not books_dir.exists():
            print(f"âŒ ä¹¦ç±ç›®å½•ä¸å­˜åœ¨: {books_dir}")
            return False
        
        # è·å–æ‰€æœ‰txtæ–‡ä»¶
        txt_files = sorted(books_dir.glob("*.txt"))
        
        if not txt_files:
            print(f"âŒ åœ¨ {books_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•txtæ–‡ä»¶")
            return False
        
        print(f"ğŸ“š å¼€å§‹åˆå¹¶ {len(txt_files)} ä¸ªä¹¦ç±æ–‡ä»¶...")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")
        print("=" * 60)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # åˆ é™¤æ—§æ–‡ä»¶
        if output_path.exists():
            output_path.unlink()
        
        total_size = 0
        start_time = time.time()
        
        # åˆå¹¶æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as outfile:
            for i, txt_file in enumerate(txt_files, 1):
                try:
                    # æ·»åŠ ä¹¦ç±åˆ†éš”ç¬¦
                    book_marker = f"=== {txt_file.name} ==="
                    outfile.write(f"\n\n{book_marker}\n")
                    
                    # è¯»å–å¹¶å†™å…¥å†…å®¹
                    with open(txt_file, 'r', encoding='utf-8') as infile:
                        content = infile.read()
                        outfile.write(content)
                        
                        file_size = len(content.encode('utf-8'))
                        total_size += file_size
                        
                        progress = (i / len(txt_files)) * 100
                        print(f"  [{progress:5.1f}%] å·²åˆå¹¶: {txt_file.name} ({file_size/1024:.1f} KB)")
                        
                except Exception as e:
                    print(f"  âŒ é”™è¯¯å¤„ç† {txt_file.name}: {e}")
                    continue
        
        # æ˜¾ç¤ºç»“æœ
        end_time = time.time()
        processing_time = end_time - start_time
        actual_size = output_path.stat().st_size / 1024 / 1024  # MB
        
        print("\n" + "=" * 60)
        print("âœ… åˆå¹¶å®Œæˆ!")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - åˆå¹¶ä¹¦ç±: {len(txt_files)} æœ¬")
        print(f"   - æ–‡ä»¶å¤§å°: {actual_size:.1f} MB")
        print(f"   - å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
        print(f"   - è¾“å‡ºæ–‡ä»¶: {output_path}")
        
        return True
    
    def list_collections(self):
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ä¹¦ç±é›†åˆ"""
        collections = self.config.get("book_collections", {})
        
        print("ğŸ“š å¯ç”¨çš„ä¹¦ç±é›†åˆ:")
        print("=" * 60)
        
        for name, info in collections.items():
            books_count = len(info.get("books", []))
            target_size = info.get("target_size_mb", 0)
            description = info.get("description", "")
            
            print(f"ğŸ”– {name}")
            print(f"   æè¿°: {description}")
            print(f"   ä¹¦ç±æ•°é‡: {books_count} æœ¬")
            print(f"   ç›®æ ‡å¤§å°: {target_size} MB")
            print()
    
    def update_from_online(self, source_name: str = "gutenberg_popular") -> bool:
        """ä»åœ¨çº¿æºæ›´æ–°ä¹¦ç±åˆ—è¡¨ï¼ˆé¢„ç•™åŠŸèƒ½ï¼‰
        
        Args:
            source_name: åœ¨çº¿æºåç§°
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        print(f"ğŸŒ ä»åœ¨çº¿æºæ›´æ–°ä¹¦ç±åˆ—è¡¨: {source_name}")
        print("âš ï¸  æ­¤åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­...")
        return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Project Gutenberg ç»Ÿä¸€ä¹¦ç±ä¸‹è½½å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  %(prog)s --list                    # åˆ—å‡ºæ‰€æœ‰å¯ç”¨é›†åˆ
  %(prog)s --download essential      # ä¸‹è½½åŸºç¡€é›†åˆ
  %(prog)s --download mega           # ä¸‹è½½å¤§å‹é›†åˆ
  %(prog)s --merge                   # åˆå¹¶æ‰€æœ‰ä¹¦ç±
  %(prog)s --download essential --merge  # ä¸‹è½½å¹¶åˆå¹¶
        """
    )
    
    parser.add_argument("--config", "-c", 
                       default="book_catalog.json",
                       help="é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: book_catalog.json)")
    
    parser.add_argument("--list", "-l", 
                       action="store_true",
                       help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ä¹¦ç±é›†åˆ")
    
    parser.add_argument("--download", "-d",
                       metavar="COLLECTION",
                       help="ä¸‹è½½æŒ‡å®šçš„ä¹¦ç±é›†åˆ (essential, extended, mega, large_works)")
    
    parser.add_argument("--merge", "-m",
                       action="store_true", 
                       help="åˆå¹¶æ‰€æœ‰ä¸‹è½½çš„ä¹¦ç±ä¸ºä¸€ä¸ªæ–‡ä»¶")
    
    parser.add_argument("--output", "-o",
                       metavar="FILE",
                       help="åˆå¹¶è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # åˆ›å»ºä¸‹è½½å™¨
    try:
        downloader = GutenbergDownloader(args.config)
    except SystemExit:
        return 1
    
    # æ‰§è¡Œæ“ä½œ
    if args.list:
        downloader.list_collections()
        return 0
    
    if args.download:
        success = downloader.download_collection(args.download)
        if not success:
            return 1
    
    if args.merge:
        success = downloader.merge_books(args.output)
        if not success:
            return 1
    
    if not any([args.list, args.download, args.merge]):
        parser.print_help()
        return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(main())